{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Starting from `01:22:00`\n",
    "\n",
    "ðŸŸ¢ **Run this Notebook with an Nvidia GPU (Ada, Ampere or later)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024  # maximum sequence length\n",
    "    vocab_size: int = (\n",
    "        50257  # number of tokens (50k BPE merges + 256 byte tokens + 1 <|endoftext|> token)\n",
    "    )\n",
    "    n_layer: int = 12  # number of layers\n",
    "    n_head: int = 12  # number of heads\n",
    "    n_embd: int = 768  # embedding size\n",
    "\n",
    "\n",
    "# Multi-Head Attention (in a single class)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # Bias (or mask)\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch size, sequence length, embedding size (n_embd)\n",
    "        B, T, C = x.size()\n",
    "        # Query, Key, Value (extract them from c_attn)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        # n_head is treated as a batch dimension\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        # Attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (C // self.n_head) ** 0.5)\n",
    "        # apply the mask\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        # apply the softmax\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        # apply the attention\n",
    "        y = att @ v\n",
    "        # transpose and reshape\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            {\n",
    "                # token embedding\n",
    "                \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                # positional embedding\n",
    "                \"wpe\": nn.Embedding(config.block_size, config.n_embd),\n",
    "                # transformer layers\n",
    "                \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                # final layer norm (Before the Linear layer)\n",
    "                \"ln_f\": nn.LayerNorm(config.n_embd),\n",
    "            }\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "            # 1 / sqrt(2 * number of residual layers) note that each layer has two residual connections\n",
    "            std *= (2 * self.config.n_layer) ** -0.5\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Shape of idx is (B, T) (Batch size, Sequence length)\n",
    "        B, T = idx.size()\n",
    "        assert (\n",
    "            T <= self.config.block_size\n",
    "        ), f\"Can't forward a sequence of length {T} longer than the block size of {self.config.block_size}\"\n",
    "        # Get the token embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # Shape is (T)\n",
    "        pos_emb = self.transformer.wpe(pos)  # Shape is (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx)  # Shape is (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb  # Shape is (B, T, n_embd) Broadcasting in addition\n",
    "        # Forward pass through the transformer layers\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # Final layer norm\n",
    "        x = self.transformer.ln_f(x)\n",
    "        # Get the logits\n",
    "        logits = self.lm_head(x)  # Shape is (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1)\n",
    "            )  # (B * T, vocab_size)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open(\"input.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position + (B * T) + 1]\n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "        self.current_position += B * T\n",
    "        # reset if we reach the end\n",
    "        if self.current_position + (B * T) + 1 > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define times dict (will use this for visualization later)\n",
    "times_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Cores and TF32 Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorcore is an instruction in Nvidia GPUs that can perform matrix multiplication on 4x4x4 matrices.\n",
    "![Tensorcore](https://leimao.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/turing-tensor-core-math.png)\n",
    "\n",
    "- `TF32` has less mantissa bits than `FP32` (10 bits vs 23 bits), but it has the same exponent bits (8 bits).\n",
    "- We lose that precision internally, but inputs, outputs, and accumulators are still stored in `FP32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 10.957387924194336, Time: 753.004ms\n",
      "Step 1, Loss: 9.538019180297852, Time: 518.327ms\n",
      "Step 2, Loss: 8.89681339263916, Time: 517.775ms\n",
      "Step 3, Loss: 8.577643394470215, Time: 518.211ms\n",
      "Step 4, Loss: 8.484574317932129, Time: 517.659ms\n",
      "Step 5, Loss: 8.364702224731445, Time: 517.588ms\n",
      "Step 6, Loss: 8.3416109085083, Time: 517.478ms\n",
      "Step 7, Loss: 8.029879570007324, Time: 517.765ms\n",
      "Step 8, Loss: 7.7533860206604, Time: 517.852ms\n",
      "Step 9, Loss: 7.706841468811035, Time: 517.425ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(\"cuda\")\n",
    "times = []\n",
    "train_loader = DataLoaderLite(8, 1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Step {i}, Loss: {loss.item()}, Time: {(t1 - t0)* 1000:.3f}ms\")\n",
    "    times.append(t1 - t0)\n",
    "\n",
    "times_dict[\"FP32\"] = sum(times[1:]) / len(times[1:]) * 1000\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FP32': 517.7866882748075}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 10.894708633422852, Time: 187.335ms\n",
      "Step 1, Loss: 9.578577995300293, Time: 178.406ms\n",
      "Step 2, Loss: 8.811702728271484, Time: 173.367ms\n",
      "Step 3, Loss: 8.590603828430176, Time: 174.037ms\n",
      "Step 4, Loss: 8.454463005065918, Time: 173.222ms\n",
      "Step 5, Loss: 8.378583908081055, Time: 174.256ms\n",
      "Step 6, Loss: 8.316251754760742, Time: 173.203ms\n",
      "Step 7, Loss: 7.955499649047852, Time: 173.923ms\n",
      "Step 8, Loss: 7.637112140655518, Time: 173.480ms\n",
      "Step 9, Loss: 7.587352275848389, Time: 173.849ms\n"
     ]
    }
   ],
   "source": [
    "# Set matmul to TF32 (it will work for Ampere GPUs)\n",
    "# The improvment will not improve by the expected amount due to memory bandwidth limitations\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(\"cuda\")\n",
    "times = []\n",
    "train_loader = DataLoaderLite(8, 1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Step {i}, Loss: {loss.item()}, Time: {(t1 - t0 )* 1000:.3f}ms\")\n",
    "    times.append(t1 - t0)\n",
    "\n",
    "times_dict[\"TF32\"] = sum(times[1:]) / len(times[1:]) * 1000\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 10.991813659667969, Time: 324.592ms\n",
      "Step 1, Loss: 9.611948013305664, Time: 160.072ms\n",
      "Step 2, Loss: 9.148838996887207, Time: 159.672ms\n",
      "Step 3, Loss: 8.70913314819336, Time: 160.351ms\n",
      "Step 4, Loss: 8.619184494018555, Time: 159.592ms\n",
      "Step 5, Loss: 8.528640747070312, Time: 160.381ms\n",
      "Step 6, Loss: 8.452939987182617, Time: 159.635ms\n",
      "Step 7, Loss: 8.09904670715332, Time: 160.069ms\n",
      "Step 8, Loss: 7.770732879638672, Time: 159.700ms\n",
      "Step 9, Loss: 7.719017028808594, Time: 159.965ms\n"
     ]
    }
   ],
   "source": [
    "# Set matmul to TF32 (it will work for Ampere GPUs)\n",
    "# The improvment will not improve by the expected 8x due to memory bandwidth\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(\"cuda\")\n",
    "times = []\n",
    "train_loader = DataLoaderLite(8, 1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Step {i}, Loss: {loss.item()}, Time: {(t1 - t0 )* 1000:.3f}ms\")\n",
    "    times.append(t1 - t0)\n",
    "\n",
    "times_dict[\"Mixed Precision\"] = sum(times[1:]) / len(times[1:]) * 1000\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 11.022022247314453, Time: 33390.031ms\n",
      "Step 1, Loss: 9.615649223327637, Time: 69.568ms\n",
      "Step 2, Loss: 9.058612823486328, Time: 68.612ms\n",
      "Step 3, Loss: 8.629047393798828, Time: 68.395ms\n",
      "Step 4, Loss: 8.553762435913086, Time: 68.635ms\n",
      "Step 5, Loss: 8.425819396972656, Time: 68.258ms\n",
      "Step 6, Loss: 8.368026733398438, Time: 68.849ms\n",
      "Step 7, Loss: 8.065177917480469, Time: 69.412ms\n",
      "Step 8, Loss: 7.799077033996582, Time: 69.380ms\n",
      "Step 9, Loss: 7.744631290435791, Time: 68.663ms\n"
     ]
    }
   ],
   "source": [
    "# Set matmul to TF32 (it will work for Ampere GPUs)\n",
    "# The improvment will not improve by the expected 8x due to memory bandwidth\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(\"cuda\")\n",
    "model = torch.compile(model)\n",
    "times = []\n",
    "train_loader = DataLoaderLite(8, 1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Step {i}, Loss: {loss.item()}, Time: {(t1 - t0 )* 1000:.3f}ms\")\n",
    "    times.append(t1 - t0)\n",
    "\n",
    "times_dict[\"torch.compile\"] = sum(times[1:]) / len(times[1:]) * 1000\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention (in a single class)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # Bias (or mask)\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch size, sequence length, embedding size (n_embd)\n",
    "        B, T, C = x.size()\n",
    "        # Query, Key, Value (extract them from c_attn)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        # n_head is treated as a batch dimension\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        # # Attention (Comment this since we will use flash attention)\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / (C // self.n_head) ** 0.5)\n",
    "        # # apply the mask\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        # # apply the softmax\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # apply the attention\n",
    "        # y = att @ v\n",
    "\n",
    "        # Flash attention (torch.compile will compile this into flash attention)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        # transpose and reshape\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            {\n",
    "                # token embedding\n",
    "                \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                # positional embedding\n",
    "                \"wpe\": nn.Embedding(config.block_size, config.n_embd),\n",
    "                # transformer layers\n",
    "                \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                # final layer norm (Before the Linear layer)\n",
    "                \"ln_f\": nn.LayerNorm(config.n_embd),\n",
    "            }\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "            # 1 / sqrt(2 * number of residual layers) note that each layer has two residual connections\n",
    "            std *= (2 * self.config.n_layer) ** -0.5\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Shape of idx is (B, T) (Batch size, Sequence length)\n",
    "        B, T = idx.size()\n",
    "        assert (\n",
    "            T <= self.config.block_size\n",
    "        ), f\"Can't forward a sequence of length {T} longer than the block size of {self.config.block_size}\"\n",
    "        # Get the token embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # Shape is (T)\n",
    "        pos_emb = self.transformer.wpe(pos)  # Shape is (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx)  # Shape is (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb  # Shape is (B, T, n_embd) Broadcasting in addition\n",
    "        # Forward pass through the transformer layers\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # Final layer norm\n",
    "        x = self.transformer.ln_f(x)\n",
    "        # Get the logits\n",
    "        logits = self.lm_head(x)  # Shape is (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1)\n",
    "            )  # (B * T, vocab_size)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 11.034364700317383, Time: 20280.728ms\n",
      "Step 1, Loss: 9.514680862426758, Time: 54.144ms\n",
      "Step 2, Loss: 8.77264404296875, Time: 51.440ms\n",
      "Step 3, Loss: 8.600730895996094, Time: 51.309ms\n",
      "Step 4, Loss: 8.409421920776367, Time: 50.623ms\n",
      "Step 5, Loss: 8.384791374206543, Time: 53.411ms\n",
      "Step 6, Loss: 8.319089889526367, Time: 51.133ms\n",
      "Step 7, Loss: 7.983427047729492, Time: 50.501ms\n",
      "Step 8, Loss: 7.694617748260498, Time: 50.589ms\n",
      "Step 9, Loss: 7.698033332824707, Time: 50.790ms\n"
     ]
    }
   ],
   "source": [
    "# Set matmul to TF32 (it will work for Ampere GPUs)\n",
    "# The improvment will not improve by the expected 8x due to memory bandwidth\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(\"cuda\")\n",
    "model = torch.compile(model)\n",
    "times = []\n",
    "train_loader = DataLoaderLite(8, 1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Step {i}, Loss: {loss.item()}, Time: {(t1 - t0 )* 1000:.3f}ms\")\n",
    "    times.append(t1 - t0)\n",
    "\n",
    "times_dict[\"flash attention\"] = sum(times[1:]) / len(times[1:]) * 1000\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nice / Ugly Numbers (Vocab size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab size is originally `50257` which is an ugly number (not too much powers of 2's)\n",
    "- we change it to `50304` by adding extra (useless) tokens\n",
    "- Theorically, it increases the computation. but practically, it's better for the GPU becasue it's a nice number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 11.024402618408203, Time: 21225.092ms\n",
      "Step 1, Loss: 9.590682983398438, Time: 49.706ms\n",
      "Step 2, Loss: 9.094247817993164, Time: 49.854ms\n",
      "Step 3, Loss: 8.745124816894531, Time: 49.994ms\n",
      "Step 4, Loss: 8.642881393432617, Time: 49.649ms\n",
      "Step 5, Loss: 8.518627166748047, Time: 52.899ms\n",
      "Step 6, Loss: 8.399345397949219, Time: 50.822ms\n",
      "Step 7, Loss: 8.053958892822266, Time: 49.612ms\n",
      "Step 8, Loss: 7.75006103515625, Time: 50.741ms\n",
      "Step 9, Loss: 7.73458194732666, Time: 50.609ms\n"
     ]
    }
   ],
   "source": [
    "# Set matmul to TF32 (it will work for Ampere GPUs)\n",
    "# The improvment will not improve by the expected 8x due to memory bandwidth\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(\"cuda\")\n",
    "model = torch.compile(model)\n",
    "times = []\n",
    "train_loader = DataLoaderLite(8, 1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Step {i}, Loss: {loss.item()}, Time: {(t1 - t0 )* 1000:.3f}ms\")\n",
    "    times.append(t1 - t0)\n",
    "\n",
    "times_dict[\"nice numbers\"] = sum(times[1:]) / len(times[1:]) * 1000\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the time improvement with the different techniques\n",
    "plt.figure(figsize=(10, 5))\n",
    "i = 0\n",
    "for k, v in times_dict.items():\n",
    "    plt.bar(k, v)\n",
    "    # on top of each bar write the percentage of reduction from the previous one\n",
    "    if i == 0:\n",
    "        prev = v\n",
    "    else:\n",
    "        plt.text(i, v + 20, f\"-{(prev - v) / prev * 100:.1f}%\", ha=\"center\")\n",
    "        prev = v\n",
    "    i += 1\n",
    "\n",
    "speedup = times_dict[list(times_dict.keys())[0]] / v\n",
    "# plot the total reduction from last to first\n",
    "plt.ylabel(\"Time (ms)\")\n",
    "plt.title(f\"{speedup:0.1f}x Speedup using {torch.cuda.get_device_name()} GPU\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
