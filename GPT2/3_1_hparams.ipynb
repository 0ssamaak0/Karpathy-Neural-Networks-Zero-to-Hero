{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from [02:14:55](https://www.youtube.com/watch?v=l8pRSuU81PU&t=8095s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024  # maximum sequence length\n",
    "    vocab_size: int = (\n",
    "        50257  # number of tokens (50k BPE merges + 256 byte tokens + 1 <|endoftext|> token)\n",
    "    )\n",
    "    n_layer: int = 12  # number of layers\n",
    "    n_head: int = 12  # number of heads\n",
    "    n_embd: int = 768  # embedding size\n",
    "\n",
    "\n",
    "# Multi-Head Attention (in a single class)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # Bias (or mask)\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch size, sequence length, embedding size (n_embd)\n",
    "        B, T, C = x.size()\n",
    "        # Query, Key, Value (extract them from c_attn)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        # n_head is treated as a batch dimension\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        # # Attention (Comment this since we will use flash attention)\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / (C // self.n_head) ** 0.5)\n",
    "        # # apply the mask\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        # # apply the softmax\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # apply the attention\n",
    "        # y = att @ v\n",
    "\n",
    "        # Flash attention (torch.compile will compile this into flash attention)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        # transpose and reshape\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            {\n",
    "                # token embedding\n",
    "                \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                # positional embedding\n",
    "                \"wpe\": nn.Embedding(config.block_size, config.n_embd),\n",
    "                # transformer layers\n",
    "                \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                # final layer norm (Before the Linear layer)\n",
    "                \"ln_f\": nn.LayerNorm(config.n_embd),\n",
    "            }\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "            # 1 / sqrt(2 * number of residual layers) note that each layer has two residual connections\n",
    "            std *= (2 * self.config.n_layer) ** -0.5\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Shape of idx is (B, T) (Batch size, Sequence length)\n",
    "        B, T = idx.size()\n",
    "        assert (\n",
    "            T <= self.config.block_size\n",
    "        ), f\"Can't forward a sequence of length {T} longer than the block size of {self.config.block_size}\"\n",
    "        # Get the token embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # Shape is (T)\n",
    "        pos_emb = self.transformer.wpe(pos)  # Shape is (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx)  # Shape is (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb  # Shape is (B, T, n_embd) Broadcasting in addition\n",
    "        # Forward pass through the transformer layers\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # Final layer norm\n",
    "        x = self.transformer.ln_f(x)\n",
    "        # Get the logits\n",
    "        logits = self.lm_head(x)  # Shape is (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1)\n",
    "            )  # (B * T, vocab_size)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open(\"input.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position + (B * T) + 1]\n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "        self.current_position += B * T\n",
    "        # reset if we reach the end\n",
    "        if self.current_position + (B * T) + 1 > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matmul to TF32 (it will work for Ampere GPUs)\n",
    "# The improvment will not improve by the expected 8x due to memory bandwidth\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(\"cuda\")\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdamW betas & gradient norm clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 10.961965560913086, norm: 26.8432, Time: 21818.676ms\n",
      "Step 1, Loss: 9.588728904724121, norm: 5.6657, Time: 223.082ms\n",
      "Step 2, Loss: 9.100929260253906, norm: 5.0262, Time: 225.568ms\n",
      "Step 3, Loss: 8.699085235595703, norm: 3.5660, Time: 222.778ms\n",
      "Step 4, Loss: 8.48172664642334, norm: 2.7194, Time: 227.257ms\n",
      "Step 5, Loss: 8.413750648498535, norm: 2.6810, Time: 235.440ms\n",
      "Step 6, Loss: 8.324607849121094, norm: 2.3129, Time: 225.651ms\n",
      "Step 7, Loss: 7.969107627868652, norm: 1.7272, Time: 225.883ms\n",
      "Step 8, Loss: 7.668796539306641, norm: 1.6388, Time: 227.598ms\n",
      "Step 9, Loss: 7.602723121643066, norm: 1.4358, Time: 231.248ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "train_loader = DataLoaderLite(8, 1024)\n",
    "# set betas to (0.9, 0.95) and eps to 1e-8\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    # clip gradients norm (to avoid very large gradients)\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(\n",
    "        f\"Step {i}, Loss: {loss.item()}, norm: {norm:.4f}, Time: {(t1 - t0 )* 1000:.3f}ms\"\n",
    "    )\n",
    "    times.append(t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cosine with warmup](https://miro.medium.com/v2/resize:fit:1400/1*BJCssPOCn4u__NoAZs392w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andrej's implementation\n",
    "def get_lr(it):\n",
    "    # 1) Linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    coeff = 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch implementation (combines linear scheduling and cosine annealing)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=6e-4)\n",
    "\n",
    "\n",
    "def get_lr_pytorch():\n",
    "    # 1) Warmup Scheduler (Linear)\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer=optimizer,\n",
    "        start_factor=1 / warmup_steps,\n",
    "        end_factor=1,\n",
    "        total_iters=warmup_steps,\n",
    "    )\n",
    "\n",
    "    # 2) Cosine Scheduler\n",
    "    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=(max_steps - warmup_steps), eta_min=min_lr\n",
    "    )\n",
    "\n",
    "    # 3) Combine Both Schedulers\n",
    "    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer=optimizer,\n",
    "        schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "        milestones=[warmup_steps],\n",
    "    )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd469913a60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Both graphs should be identical\n",
    "lrs = []\n",
    "lrs_pytorch = []\n",
    "scheduler = get_lr_pytorch()\n",
    "\n",
    "for i in range(max_steps):\n",
    "    lrs.append(get_lr(i))\n",
    "    scheduler.step()\n",
    "    lrs_pytorch.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "# Plot the graphs\n",
    "plt.plot(lrs, label=\"Andrej's Implementation\")\n",
    "plt.plot(lrs_pytorch, label=\"PyTorch Implementation\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 7.1741414070129395, lr: 6.0000e-05, norm: 1.7669, Time: 227.182ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 7.085329532623291, lr: 1.2000e-04, norm: 1.7881, Time: 223.317ms\n",
      "Step 2, Loss: 6.739221572875977, lr: 1.8000e-04, norm: 1.5870, Time: 224.496ms\n",
      "Step 3, Loss: 6.669004440307617, lr: 2.4000e-04, norm: 1.4240, Time: 230.019ms\n",
      "Step 4, Loss: 6.613946914672852, lr: 3.0000e-04, norm: 1.5314, Time: 236.721ms\n",
      "Step 5, Loss: 6.7087883949279785, lr: 3.6000e-04, norm: 1.5298, Time: 230.985ms\n",
      "Step 6, Loss: 6.906996726989746, lr: 4.2000e-04, norm: 2.5751, Time: 226.552ms\n",
      "Step 7, Loss: 6.570232391357422, lr: 4.8000e-04, norm: 1.3464, Time: 224.849ms\n",
      "Step 8, Loss: 6.639470100402832, lr: 5.4000e-04, norm: 5.3371, Time: 235.049ms\n",
      "Step 9, Loss: 6.580402374267578, lr: 6.0000e-04, norm: 2.9929, Time: 235.152ms\n",
      "Step 10, Loss: 6.719510555267334, lr: 6.0000e-04, norm: 2.4282, Time: 234.439ms\n",
      "Step 11, Loss: 6.715445518493652, lr: 5.9917e-04, norm: 2.6444, Time: 228.423ms\n",
      "Step 12, Loss: 6.710544586181641, lr: 5.9668e-04, norm: 1.5415, Time: 233.490ms\n",
      "Step 13, Loss: 6.55912971496582, lr: 5.9254e-04, norm: 1.1763, Time: 234.992ms\n",
      "Step 14, Loss: 6.520519256591797, lr: 5.8679e-04, norm: 1.1781, Time: 231.346ms\n",
      "Step 15, Loss: 6.308019638061523, lr: 5.7945e-04, norm: 1.0814, Time: 232.578ms\n",
      "Step 16, Loss: 6.403852462768555, lr: 5.7057e-04, norm: 1.1557, Time: 232.585ms\n",
      "Step 17, Loss: 6.453081130981445, lr: 5.6021e-04, norm: 0.8425, Time: 229.695ms\n",
      "Step 18, Loss: 6.35929012298584, lr: 5.4843e-04, norm: 1.1405, Time: 236.314ms\n",
      "Step 19, Loss: 6.192653179168701, lr: 5.3531e-04, norm: 1.1567, Time: 231.835ms\n",
      "Step 20, Loss: 6.2637200355529785, lr: 5.2092e-04, norm: 1.0543, Time: 230.432ms\n",
      "Step 21, Loss: 6.127110958099365, lr: 5.0535e-04, norm: 1.0925, Time: 235.268ms\n",
      "Step 22, Loss: 6.233137130737305, lr: 4.8870e-04, norm: 0.9713, Time: 232.896ms\n",
      "Step 23, Loss: 6.060192108154297, lr: 4.7107e-04, norm: 0.9292, Time: 231.718ms\n",
      "Step 24, Loss: 6.070716381072998, lr: 4.5258e-04, norm: 0.8846, Time: 234.581ms\n",
      "Step 25, Loss: 6.110913276672363, lr: 4.3332e-04, norm: 0.8180, Time: 232.712ms\n",
      "Step 26, Loss: 6.454296588897705, lr: 4.1343e-04, norm: 1.1520, Time: 232.541ms\n",
      "Step 27, Loss: 6.303739070892334, lr: 3.9303e-04, norm: 1.1481, Time: 233.496ms\n",
      "Step 28, Loss: 6.535346984863281, lr: 3.7224e-04, norm: 0.9887, Time: 232.666ms\n",
      "Step 29, Loss: 6.289669036865234, lr: 3.5118e-04, norm: 1.0938, Time: 233.146ms\n",
      "Step 30, Loss: 6.268405914306641, lr: 3.3000e-04, norm: 0.8659, Time: 231.621ms\n",
      "Step 31, Loss: 6.265917778015137, lr: 3.0882e-04, norm: 1.1533, Time: 232.176ms\n",
      "Step 32, Loss: 6.145758628845215, lr: 2.8776e-04, norm: 1.1413, Time: 233.111ms\n",
      "Step 33, Loss: 6.404779434204102, lr: 2.6697e-04, norm: 1.2788, Time: 230.722ms\n",
      "Step 34, Loss: 6.270020484924316, lr: 2.4657e-04, norm: 1.0682, Time: 231.101ms\n",
      "Step 35, Loss: 6.175246238708496, lr: 2.2668e-04, norm: 0.8202, Time: 234.830ms\n",
      "Step 36, Loss: 6.2279558181762695, lr: 2.0742e-04, norm: 1.1748, Time: 231.973ms\n",
      "Step 37, Loss: 6.245031356811523, lr: 1.8893e-04, norm: 1.0376, Time: 234.038ms\n",
      "Step 38, Loss: 6.008410930633545, lr: 1.7130e-04, norm: 1.0935, Time: 237.869ms\n",
      "Step 39, Loss: 6.068855285644531, lr: 1.5465e-04, norm: 1.0034, Time: 235.771ms\n",
      "Step 40, Loss: 6.321634769439697, lr: 1.3908e-04, norm: 1.3673, Time: 233.465ms\n",
      "Step 41, Loss: 6.068855285644531, lr: 1.2469e-04, norm: 1.2404, Time: 232.404ms\n",
      "Step 42, Loss: 6.140230178833008, lr: 1.1157e-04, norm: 1.4791, Time: 236.489ms\n",
      "Step 43, Loss: 5.842097759246826, lr: 9.9787e-05, norm: 1.3136, Time: 234.493ms\n",
      "Step 44, Loss: 5.792564392089844, lr: 8.9428e-05, norm: 1.0278, Time: 233.366ms\n",
      "Step 45, Loss: 5.878620147705078, lr: 8.0553e-05, norm: 1.0874, Time: 234.838ms\n",
      "Step 46, Loss: 5.9769606590271, lr: 7.3215e-05, norm: 1.0098, Time: 236.350ms\n",
      "Step 47, Loss: 5.855202674865723, lr: 6.7460e-05, norm: 1.2814, Time: 234.797ms\n",
      "Step 48, Loss: 5.745029926300049, lr: 6.3324e-05, norm: 1.0530, Time: 232.488ms\n",
      "Step 49, Loss: 5.651922225952148, lr: 6.0832e-05, norm: 0.9619, Time: 235.264ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "train_loader = DataLoaderLite(8, 1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "losses = []\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set lr for the current iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(\n",
    "        f\"Step {step}, Loss: {loss.item()}, lr: {lr:.4e}, norm: {norm:.4f}, Time: {(t1 - t0 )* 1000:.3f}ms\"\n",
    "    )\n",
    "    times.append(t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024  # maximum sequence length\n",
    "    vocab_size: int = (\n",
    "        50257  # number of tokens (50k BPE merges + 256 byte tokens + 1 <|endoftext|> token)\n",
    "    )\n",
    "    n_layer: int = 12  # number of layers\n",
    "    n_head: int = 12  # number of heads\n",
    "    n_embd: int = 768  # embedding size\n",
    "\n",
    "\n",
    "# Multi-Head Attention (in a single class)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # Bias (or mask)\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch size, sequence length, embedding size (n_embd)\n",
    "        B, T, C = x.size()\n",
    "        # Query, Key, Value (extract them from c_attn)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        # n_head is treated as a batch dimension\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, n_head, T, hs)\n",
    "        # # Attention (Comment this since we will use flash attention)\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / (C // self.n_head) ** 0.5)\n",
    "        # # apply the mask\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        # # apply the softmax\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # apply the attention\n",
    "        # y = att @ v\n",
    "\n",
    "        # Flash attention (torch.compile will compile this into flash attention)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        # transpose and reshape\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            {\n",
    "                # token embedding\n",
    "                \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                # positional embedding\n",
    "                \"wpe\": nn.Embedding(config.block_size, config.n_embd),\n",
    "                # transformer layers\n",
    "                \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                # final layer norm (Before the Linear layer)\n",
    "                \"ln_f\": nn.LayerNorm(config.n_embd),\n",
    "            }\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "            # 1 / sqrt(2 * number of residual layers) note that each layer has two residual connections\n",
    "            std *= (2 * self.config.n_layer) ** -0.5\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Shape of idx is (B, T) (Batch size, Sequence length)\n",
    "        B, T = idx.size()\n",
    "        assert (\n",
    "            T <= self.config.block_size\n",
    "        ), f\"Can't forward a sequence of length {T} longer than the block size of {self.config.block_size}\"\n",
    "        # Get the token embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # Shape is (T)\n",
    "        pos_emb = self.transformer.wpe(pos)  # Shape is (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx)  # Shape is (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb  # Shape is (B, T, n_embd) Broadcasting in addition\n",
    "        # Forward pass through the transformer layers\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # Final layer norm\n",
    "        x = self.transformer.ln_f(x)\n",
    "        # Get the logits\n",
    "        logits = self.lm_head(x)  # Shape is (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1)\n",
    "            )  # (B * T, vocab_size)\n",
    "        return logits, loss\n",
    "\n",
    "    # The goal of the function to separate the parameters (should be weight decayed and not weight decayed)\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all params requiring grad\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, others won't\n",
    "        # all weights in matmuls and embeddings will be weight decayed\n",
    "        # biases and layernorms won't be weight decayed\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": 0.1},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        num_dcay_params = sum(p.numel() for p in decay_params)\n",
    "        num_ndcay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(\n",
    "            f\"num ecayed parameter tensors: {len(decay_params)} with {num_dcay_params:,} params\"\n",
    "        )\n",
    "        print(\n",
    "            f\"num non-decayed parameter tensors: {len(nodecay_params)} with {num_ndcay_params:,} params\"\n",
    "        )\n",
    "        fused = \"cuda\" in device\n",
    "        # fused make it faster\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=fused\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matmul to TF32 (it will work for Ampere GPUs)\n",
    "# The improvment will not improve by the expected 8x due to memory bandwidth\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(\"cuda\")\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num ecayed parameter tensors: 50 with 124,354,560 params\n",
      "num non-decayed parameter tensors: 98 with 121,344 params\n",
      "Step 0, Loss: 10.815095901489258, lr: 6.0000e-05, norm: 29.5874, Time: 5957.323ms\n",
      "Step 1, Loss: 9.53116226196289, lr: 1.2000e-04, norm: 7.9393, Time: 208.087ms\n",
      "Step 2, Loss: 8.886503219604492, lr: 1.8000e-04, norm: 3.8841, Time: 204.878ms\n",
      "Step 3, Loss: 8.761748313903809, lr: 2.4000e-04, norm: 4.9515, Time: 207.295ms\n",
      "Step 4, Loss: 9.525464057922363, lr: 3.0000e-04, norm: 8.3081, Time: 219.947ms\n",
      "Step 5, Loss: 8.886791229248047, lr: 3.6000e-04, norm: 3.7999, Time: 216.155ms\n",
      "Step 6, Loss: 8.566642761230469, lr: 4.2000e-04, norm: 2.9776, Time: 206.392ms\n",
      "Step 7, Loss: 8.15962028503418, lr: 4.8000e-04, norm: 3.0515, Time: 206.382ms\n",
      "Step 8, Loss: 7.773521423339844, lr: 5.4000e-04, norm: 4.1853, Time: 205.144ms\n",
      "Step 9, Loss: 7.561093330383301, lr: 6.0000e-04, norm: 1.7883, Time: 217.233ms\n",
      "Step 10, Loss: 7.4016032218933105, lr: 6.0000e-04, norm: 2.6062, Time: 213.023ms\n",
      "Step 11, Loss: 7.130596160888672, lr: 5.9917e-04, norm: 2.4664, Time: 209.219ms\n",
      "Step 12, Loss: 7.015944480895996, lr: 5.9668e-04, norm: 1.5905, Time: 211.053ms\n",
      "Step 13, Loss: 6.755735397338867, lr: 5.9254e-04, norm: 1.1492, Time: 210.847ms\n",
      "Step 14, Loss: 6.650586128234863, lr: 5.8679e-04, norm: 0.7998, Time: 216.193ms\n",
      "Step 15, Loss: 6.518949508666992, lr: 5.7945e-04, norm: 2.0530, Time: 213.111ms\n",
      "Step 16, Loss: 6.608120918273926, lr: 5.7057e-04, norm: 1.1548, Time: 209.151ms\n",
      "Step 17, Loss: 6.653223991394043, lr: 5.6021e-04, norm: 1.2034, Time: 210.287ms\n",
      "Step 18, Loss: 6.581635475158691, lr: 5.4843e-04, norm: 1.8648, Time: 211.146ms\n",
      "Step 19, Loss: 6.393909454345703, lr: 5.3531e-04, norm: 1.1862, Time: 215.342ms\n",
      "Step 20, Loss: 6.525697708129883, lr: 5.2092e-04, norm: 1.1824, Time: 214.700ms\n",
      "Step 21, Loss: 6.3605427742004395, lr: 5.0535e-04, norm: 2.0478, Time: 209.233ms\n",
      "Step 22, Loss: 6.4774346351623535, lr: 4.8870e-04, norm: 1.6499, Time: 212.379ms\n",
      "Step 23, Loss: 6.271306037902832, lr: 4.7107e-04, norm: 1.0254, Time: 210.926ms\n",
      "Step 24, Loss: 6.295776844024658, lr: 4.5258e-04, norm: 1.3944, Time: 217.451ms\n",
      "Step 25, Loss: 6.321747779846191, lr: 4.3332e-04, norm: 1.2716, Time: 212.533ms\n",
      "Step 26, Loss: 6.632328510284424, lr: 4.1343e-04, norm: 1.4519, Time: 211.927ms\n",
      "Step 27, Loss: 6.474213600158691, lr: 3.9303e-04, norm: 1.3842, Time: 211.875ms\n",
      "Step 28, Loss: 6.735867023468018, lr: 3.7224e-04, norm: 1.0855, Time: 214.950ms\n",
      "Step 29, Loss: 6.469518184661865, lr: 3.5118e-04, norm: 1.0727, Time: 214.802ms\n",
      "Step 30, Loss: 6.429311752319336, lr: 3.3000e-04, norm: 0.9534, Time: 211.951ms\n",
      "Step 31, Loss: 6.412330627441406, lr: 3.0882e-04, norm: 0.9097, Time: 215.209ms\n",
      "Step 32, Loss: 6.260976791381836, lr: 2.8776e-04, norm: 1.0476, Time: 210.143ms\n",
      "Step 33, Loss: 6.550978660583496, lr: 2.6697e-04, norm: 1.3363, Time: 215.223ms\n",
      "Step 34, Loss: 6.4144439697265625, lr: 2.4657e-04, norm: 1.0079, Time: 214.221ms\n",
      "Step 35, Loss: 6.334521770477295, lr: 2.2668e-04, norm: 0.9572, Time: 214.382ms\n",
      "Step 36, Loss: 6.369866371154785, lr: 2.0742e-04, norm: 1.1677, Time: 210.107ms\n",
      "Step 37, Loss: 6.386699676513672, lr: 1.8893e-04, norm: 1.1227, Time: 215.783ms\n",
      "Step 38, Loss: 6.166472911834717, lr: 1.7130e-04, norm: 1.1488, Time: 216.344ms\n",
      "Step 39, Loss: 6.237943649291992, lr: 1.5465e-04, norm: 1.1134, Time: 213.486ms\n",
      "Step 40, Loss: 6.45977783203125, lr: 1.3908e-04, norm: 1.1093, Time: 215.532ms\n",
      "Step 41, Loss: 6.280735492706299, lr: 1.2469e-04, norm: 0.9035, Time: 213.082ms\n",
      "Step 42, Loss: 6.325659275054932, lr: 1.1157e-04, norm: 1.1488, Time: 215.959ms\n",
      "Step 43, Loss: 6.039444923400879, lr: 9.9787e-05, norm: 1.1673, Time: 214.011ms\n",
      "Step 44, Loss: 5.988136291503906, lr: 8.9428e-05, norm: 0.8813, Time: 216.854ms\n",
      "Step 45, Loss: 6.093029022216797, lr: 8.0553e-05, norm: 0.9022, Time: 212.430ms\n",
      "Step 46, Loss: 6.1580657958984375, lr: 7.3215e-05, norm: 0.8057, Time: 212.580ms\n",
      "Step 47, Loss: 6.011228084564209, lr: 6.7460e-05, norm: 1.1484, Time: 212.799ms\n",
      "Step 48, Loss: 5.898131847381592, lr: 6.3324e-05, norm: 0.9004, Time: 214.679ms\n",
      "Step 49, Loss: 5.797030448913574, lr: 6.0832e-05, norm: 0.9064, Time: 215.889ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "train_loader = DataLoaderLite(8, 1024)\n",
    "optimizer = model.configure_optimizers(0.1, 6e-4, \"cuda\")\n",
    "losses = []\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set lr for the current iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(\n",
    "        f\"Step {step}, Loss: {loss.item()}, lr: {lr:.4e}, norm: {norm:.4f}, Time: {(t1 - t0 )* 1000:.3f}ms\"\n",
    "    )\n",
    "    times.append(t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524,288, grad_accum_steps: 32\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 2**19\n",
    "B = 16  # micro batch size\n",
    "T = 1024  # sequence length\n",
    "assert total_batch_size % (B * T) == 0  # total batch size must be divisible by B * T\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(\n",
    "    f\"total desired batch size: {total_batch_size:,}, grad_accum_steps: {grad_accum_steps:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num ecayed parameter tensors: 50 with 124,354,560 params\n",
      "num non-decayed parameter tensors: 98 with 121,344 params\n",
      "Step 0, Loss: 6.038639068603516, lr: 6.0000e-05, norm: 19.2737, Time: 37629.677ms\n",
      "Step 1, Loss: 5.928457260131836, lr: 1.2000e-04, norm: 15.8229, Time: 12712.087ms\n",
      "Step 2, Loss: 6.07780122756958, lr: 1.8000e-04, norm: 13.3442, Time: 12849.101ms\n",
      "Step 3, Loss: 6.032467842102051, lr: 2.4000e-04, norm: 16.1737, Time: 13016.075ms\n",
      "Step 4, Loss: 5.970197677612305, lr: 3.0000e-04, norm: 14.1002, Time: 13197.363ms\n",
      "Step 5, Loss: 5.917179107666016, lr: 3.6000e-04, norm: 18.4995, Time: 13371.280ms\n",
      "Step 6, Loss: 5.781548023223877, lr: 4.2000e-04, norm: 21.6698, Time: 13468.771ms\n",
      "Step 7, Loss: 6.006679058074951, lr: 4.8000e-04, norm: 18.5777, Time: 13610.349ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m micro_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(grad_accum_steps):\n\u001b[1;32m      9\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m train_loader\u001b[38;5;241m.\u001b[39mnext_batch()\n\u001b[0;32m---> 10\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[1;32m     12\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "times = []\n",
    "train_loader = DataLoaderLite(B, T)\n",
    "optimizer = model.configure_optimizers(0.1, 6e-4, \"cuda\")\n",
    "losses = []\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set lr for the current iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss.item())\n",
    "    print(\n",
    "        f\"Step {step}, Loss: {loss.item()}, lr: {lr:.4e}, norm: {norm:.4f}, Time: {(t1 - t0 )* 1000:.3f}ms\"\n",
    "    )\n",
    "    times.append(t1 - t0)\n",
    "    if step == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n",
      "        -0.0103, -0.0134])\n"
     ]
    }
   ],
   "source": [
    "# simple mlp\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(16, 32),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(32, 1),\n",
    ")\n",
    "torch.random.manual_seed(42)\n",
    "x = torch.randn(4, 16)\n",
    "y = torch.randn(4, 1)\n",
    "net.zero_grad()\n",
    "yhat = net(x)\n",
    "# reduction = `mean` (default) i.e., loss = 1/4 * [(y[0] - yhat[0])**2 + ... + (y[3] - yhat[3])**2]\n",
    "loss = torch.nn.functional.mse_loss(yhat, y)\n",
    "loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0598,  0.0042,  0.0167, -0.0161,  0.0235, -0.0320, -0.0311, -0.0550,\n",
      "        -0.0410, -0.0536])\n",
      "Loss is not the same (no division by 4)\n",
      "Dividing by 4: tensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n",
      "        -0.0103, -0.0134])\n"
     ]
    }
   ],
   "source": [
    "# same network with gradient accumulation (batch_size = 4, 1 micro step)\n",
    "net.zero_grad()\n",
    "for i in range(4):\n",
    "    yhat = net(x[i])\n",
    "    loss = torch.nn.functional.mse_loss(yhat, y[i])\n",
    "    loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])\n",
    "# Loss is not the same (no division by 4)\n",
    "print(f\"Dividing by 4: {net[0].weight.grad.view(-1)[:10] / 4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num ecayed parameter tensors: 50 with 124,354,560 params\n",
      "num non-decayed parameter tensors: 98 with 121,344 params\n",
      "Step 0, Loss: 5.883791923522949, lr: 6.0000e-05, norm: 0.7356, Time: 13598.408ms\n",
      "Step 1, Loss: 5.885821342468262, lr: 1.2000e-04, norm: 0.5531, Time: 13777.442ms\n",
      "Step 2, Loss: 5.879245758056641, lr: 1.8000e-04, norm: 0.6695, Time: 13902.858ms\n",
      "Step 3, Loss: 5.846792221069336, lr: 2.4000e-04, norm: 0.4080, Time: 14040.436ms\n",
      "Step 4, Loss: 5.838709354400635, lr: 3.0000e-04, norm: 0.5806, Time: 14143.143ms\n",
      "Step 5, Loss: 5.813776016235352, lr: 3.6000e-04, norm: 0.6831, Time: 14265.640ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "train_loader = DataLoaderLite(B, T)\n",
    "optimizer = model.configure_optimizers(0.1, 6e-4, \"cuda\")\n",
    "losses = []\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss /= (\n",
    "            grad_accum_steps  # divide by grad_accum_steps since the reduction is mean\n",
    "        )\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set lr for the current iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    losses.append(loss_accum)\n",
    "    print(\n",
    "        f\"Step {step}, Loss: {loss_accum}, lr: {lr:.4e}, norm: {norm:.4f}, Time: {(t1 - t0 )* 1000:.3f}ms\"\n",
    "    )\n",
    "    times.append(t1 - t0)\n",
    "    if step == 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
