{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for the Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Starter Code (from previous notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "32033\n"
     ]
    }
   ],
   "source": [
    "words = open(\"../data/names.txt\", \"r\").read().splitlines()\n",
    "print(words[:8])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "def build_dataset(words):  \n",
    "\n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "\n",
    "      ix = stoi[ch]\n",
    "\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "\n",
    "      context = context[1:] + [ix]\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 \n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden),generator=g) * (5/3) / ((n_embd * block_size) ** 0.5)\n",
    "b1 = torch.randn(n_hidden,generator=g) * 0.01 # it's actually useless\n",
    "W2 = torch.randn((n_hidden, vocab_size),generator=g) * 0.01 # Initialize to small values\n",
    "b2 = torch.randn(vocab_size,generator=g) * 0.1\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "# mean and std buffers, (not trainable)\n",
    "bnmean_running = torch.zeros((1, n_hidden)) * 0.1 + 1\n",
    "bnstd_running = torch.ones((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Utility Function to compare the Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Forward pass\n",
    "the forward back in this time is broken up into manageable chunks, so that we can compute the gradients for each layer separately (including cross entropy)\n",
    "\n",
    "This is the same forward pass as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Embeddings -----\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "# ----- Linear Layer 1 -----\n",
    "# hidden layer pre-activation\n",
    "hprebn = embcat @ W1 + b1\n",
    "# ---- BatchNorm layer ----\n",
    "# mean of hidden layer pre-activation\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim = True)\n",
    "# difference between pre-activation and mean\n",
    "bndiff = hprebn - bnmeani\n",
    "# square of difference\n",
    "bndiff2 = bndiff ** 2\n",
    "# variance of hidden layer pre-activation (Bessel's correction (n-1))\n",
    "bnvar = 1 / (n - 1) * (bndiff2).sum(0, keepdim = True)\n",
    "# get the std of hidden layer pre-activation, and invert it (remember: we need to divide by it)\n",
    "bnvar_inv = 1 / (bnvar + 1e-5) ** 0.5\n",
    "# batch norm layer output\n",
    "bnraw = bndiff * bnvar_inv\n",
    "# batch norm layer output with gain and bias (scale and shift)\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# activation function\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "# ----- Linear Layer 2 -----\n",
    "# logits\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# ----- Cross Entropy Loss -----\n",
    "# max logits\n",
    "logit_maxes = logits.max(1, keepdim = True).values\n",
    "# normalized logits (substract max logits for numerical stability)\n",
    "norm_logits = logits - logit_maxes\n",
    "# counts\n",
    "counts = norm_logits.exp()\n",
    "# counts sum\n",
    "counts_sum = counts.sum(1, keepdim = True)\n",
    "# inverse of counts sum (if it's 1 / counts_sum, we can't backpropagate through it)\n",
    "counts_sum_inv = counts_sum ** -1\n",
    "# probabilities (all counts sum to 1), notice that counts_sum_inv is broadcasted from (batch_size, 1) to (batch_size, vocab_size)\n",
    "probs = counts * counts_sum_inv\n",
    "# log probabilities of the correct next character (shape: (batch_size, vocab_size))\n",
    "logprobs = probs.log()\n",
    "# loss\n",
    "loss = -logprobs[range(n), Yb].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2968, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, bnmeani, hprebn, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Backpropagation through the whole thing manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = -(a + b + c) / 3 ➡️ dz/da = -1/3, dz/dx = 0\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0 / n\n",
    "\n",
    "# for correct prediction ✅: pass dlogprobs | if it's not correct ❌: multiply it by large number\n",
    "# y = log(x) ➡️ dy/dx = 1/x\n",
    "dprobs = dlogprobs * (probs ** (-1))\n",
    "\n",
    "# note that counts.shape = (batch_size, vocab_size) | counts_sum.shape = (batch_size, 1) ➡️ we need to sum along the vocab_size axis (dim=1)\n",
    "# z = x * y ➡️ dz/dx = y\n",
    "dcounts_sum_inv = (dprobs * counts).sum(1, keepdim = True)\n",
    "\n",
    "# z = 1 / x ➡️ dz/dx = -1 / x^2\n",
    "dcounts_sum = dcounts_sum_inv * -1 * (counts_sum ** -2)\n",
    "\n",
    "# note that counts is used twice ➡️ we need to sum gradients from both branches\n",
    "dcounts = (dcounts_sum * 1).expand_as(counts) + (dprobs * counts_sum_inv).expand_as(counts)\n",
    "\n",
    "# y = exp(x) ➡️ dy/dx = exp(x)\n",
    "dnorm_logits = dcounts  * norm_logits.exp()\n",
    "\n",
    "# the gradient on logit maxes should be zero , since it does affect probabilities hence the loss (check it yourself!)\n",
    "# z = x - y ➡️ dz/dx = 1 (remember broadcasting)\n",
    "dlogit_maxes = (dnorm_logits * -1).sum(1, keepdim = True)\n",
    "\n",
    "# note that logits is used twice ➡️ we need to sum gradients from both branches\n",
    "# for logit_maxes branch, we need to get argmaxes of logits, and then sum along the vocab_size  axis (dim=1)\n",
    "dlogits = dnorm_logits + F.one_hot(logits.max(1).indices, num_classes = logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "# z = x * y ➡️ dz/dx = y (transpose because of matrix multiplication)\n",
    "dh = dlogits @ W2.T\n",
    "\n",
    "# z = x * y ➡️ dz/dx = x (transpose because of matrix multiplication)\n",
    "dW2 = h.T @ dlogits\n",
    "\n",
    "# z = x * y + a ➡️ dz/da = 1 (sum along the batch_size axis)\n",
    "db2 = dlogits.sum(0, keepdim = False)\n",
    "\n",
    "# z = tanh(x) ➡️ dz/dx = 1 - tanh(x)^2 = 1 - z^2\n",
    "dhpreact = dh * (1.0 - h ** 2)\n",
    "\n",
    "# z = x * y + a ➡️ dz/dx = y (transpose because of matrix multiplication)\n",
    "dbngain = (dhpreact * bnraw).sum(0, keepdim = True)\n",
    "\n",
    "# z = x * y + a ➡️ dz/da = 1 (sum along the batch_size axis)\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "# z = x * y + a ➡️ dz/dy = x\n",
    "dbnraw = (bngain * dhpreact)\n",
    "\n",
    "dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim = True)\n",
    "\n",
    "# z = 1 / (x + 1e-5) ** 0.5 ➡️ dz/dx = -0.5 * (x + 1e-5) ** -1.5\n",
    "dbnvar = dbnvar_inv * -0.5 * (bnvar + 1e-5) ** -1.5\n",
    "\n",
    "# y = 1 / (n - 1) * x ➡️ dy/dx = 1 / (n - 1)\n",
    "dbndiff2 = (dbnvar * (1 / (n - 1))).expand_as(bndiff2)\n",
    "\n",
    "# z = x ** 2 ➡️ dz/dx = 2 * x\n",
    "dbndiff = (dbndiff2 * 2 * bndiff) + (dbnraw * bnvar_inv)\n",
    "\n",
    "# z = y - x ➡️ dz/dx = -1\n",
    "dbnmeani = (dbndiff * -1).sum(0, keepdim = True)\n",
    "\n",
    "# note that hprebn is used twice ➡️ we need to sum gradients from both branches\n",
    "dhprebn = dbndiff + (dbnmeani * (1 / n)).expand_as(hprebn)\n",
    "\n",
    "# z = x * y + a ➡️ dz/dx = y (transpose because of matrix multiplication)\n",
    "dembcat = dhprebn @ W1.T\n",
    "\n",
    "# z = x * y + a ➡️ dz/dx = y (transpose because of matrix multiplication)\n",
    "dW1 = embcat.T @ dhprebn\n",
    "\n",
    "# z = x * y + a ➡️ dz/da = 1 (sum along the batch_size axis)\n",
    "db1 = dhprebn.sum(0, keepdim = False)\n",
    "\n",
    "# just reshape the gradient to the shape of the embedding\n",
    "demb = dembcat.view_as(emb)\n",
    "\n",
    "# we need to undo the embedding lookup (the indexing)\n",
    "dC = torch.zeros_like(C)\n",
    "# iterate over Xb rows\n",
    "for k in range(Xb.shape[0]):\n",
    "    # iterate over Xb columns\n",
    "    for j in range(Xb.shape[1]):\n",
    "        # get the index of the current character\n",
    "        ix = Xb[k, j]\n",
    "        # add the gradient to the corresponding row of the gradient matrix\n",
    "        dC[ix] += demb[k, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shapetorch.Size([27, 10])\n",
      "emb shapetorch.Size([32, 3, 10])\n",
      "Xb shapetorch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Change this with the shapes of the tensors you're currently working with\n",
    "print(f\"C shape{C.shape}\")\n",
    "print(f\"emb shape{emb.shape}\")\n",
    "print(f\"Xb shape{Xb.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "bngain          | exact: False | approximate: True  | maxdiff: 1.7462298274040222e-10\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-10\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-10\n",
      "bndiff2         | exact: False | approximate: True  | maxdiff: 3.637978807091713e-12\n",
      "bndiff          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "bnmeani         | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-10\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 1.7462298274040222e-10\n",
      "W1              | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.2014213502407074e-10\n",
      "emb             | exact: False | approximate: True  | maxdiff: 1.7462298274040222e-10\n",
      "C               | exact: False | approximate: True  | maxdiff: 3.637978807091713e-10\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Backprop through cross_entropy in one expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_fast: 3.296769618988037 | dif: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(f\"loss_fast: {loss_fast} | dif: {loss_fast - loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-09\n"
     ]
    }
   ],
   "source": [
    "# (check mathematical explaination in video)\n",
    "dlogits = F.softmax(logits, dim = 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the gradients (dlogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0434, 0.0369, 0.0356, 0.0364, 0.0329, 0.0465, 0.0415, 0.0422, 0.0360,\n",
      "        0.0344, 0.0364, 0.0332, 0.0411, 0.0353, 0.0327, 0.0352, 0.0318, 0.0318,\n",
      "        0.0330, 0.0446, 0.0408, 0.0316, 0.0383, 0.0391, 0.0435, 0.0349, 0.0311],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0434,  0.0369,  0.0356,  0.0364,  0.0329,  0.0465,  0.0415,  0.0422,\n",
      "        -0.9640,  0.0344,  0.0364,  0.0332,  0.0411,  0.0353,  0.0327,  0.0352,\n",
      "         0.0318,  0.0318,  0.0330,  0.0446,  0.0408,  0.0316,  0.0383,  0.0391,\n",
      "         0.0435,  0.0349,  0.0311], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "plt.imshow(dlogits.data, cmap = 'gray', interpolation = 'nearest');\n",
    "\n",
    "# notice that they are equal, but correct posision is almost -1\n",
    "print(F.softmax(logits, 1)[0])\n",
    "print(dlogits[0] * n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Batch Norm Layer Backward Pass in one Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n"
     ]
    }
   ],
   "source": [
    "# calculate hprebn given hpreact (check mathematical explaination in video)\n",
    "dhprebn = bngain * bnvar_inv / n * (n * dhpreact - dhpreact.sum(0) - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss 3.296769618988037\n",
      "Step 10000, Loss 2.4469268321990967\n",
      "Step 20000, Loss 2.0859169960021973\n",
      "Step 30000, Loss 2.388929605484009\n",
      "Step 40000, Loss 2.2814080715179443\n",
      "Step 50000, Loss 2.1699628829956055\n",
      "Step 60000, Loss 2.2227632999420166\n",
      "Step 70000, Loss 2.5007517337799072\n",
      "Step 80000, Loss 2.1813900470733643\n",
      "Step 90000, Loss 2.4679534435272217\n",
      "Step 100000, Loss 2.095520257949829\n",
      "Step 110000, Loss 1.7801880836486816\n",
      "Step 120000, Loss 2.076627731323242\n",
      "Step 130000, Loss 2.4670305252075195\n",
      "Step 140000, Loss 1.8943418264389038\n",
      "Step 150000, Loss 2.13836932182312\n",
      "Step 160000, Loss 2.0354418754577637\n",
      "Step 170000, Loss 2.303077220916748\n",
      "Step 180000, Loss 1.9737322330474854\n",
      "Step 190000, Loss 1.993587613105774\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "n_embd = 10 \n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden),generator=g) * (5/3) / ((n_embd * block_size) ** 0.5)\n",
    "b1 = torch.randn(n_hidden,generator=g) * 0.01 # it's actually useless\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),generator=g) * 0.01 # Initialize to small values\n",
    "b2 = torch.randn(vocab_size,generator=g) * 0.1\n",
    "# BatchNorm Parameters\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "# mean and std buffers, (not trainable)\n",
    "bnmean_running = torch.zeros((1, n_hidden)) * 0.1 + 1\n",
    "bnstd_running = torch.ones((1, n_hidden)) * 0.1\n",
    "\n",
    "params = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size \n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # ----- Embeddings -----\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "    # ----- Linear Layer 1 -----\n",
    "    # hidden layer pre-activation\n",
    "    hprebn = embcat @ W1 + b1\n",
    "    # ---- BatchNorm layer ----\n",
    "    # mean of hidden layer pre-activation\n",
    "    bnmeani = 1 / n * hprebn.sum(0, keepdim = True)\n",
    "    # difference between pre-activation and mean\n",
    "    bndiff = hprebn - bnmeani\n",
    "    # square of difference\n",
    "    bndiff2 = bndiff ** 2\n",
    "    # variance of hidden layer pre-activation (Bessel's correction (n-1))\n",
    "    bnvar = 1 / (n - 1) * (bndiff2).sum(0, keepdim = True)\n",
    "    # get the std of hidden layer pre-activation, and invert it (remember: we need to divide by it)\n",
    "    bnvar_inv = 1 / (bnvar + 1e-5) ** 0.5\n",
    "    # batch norm layer output\n",
    "    bnraw = bndiff * bnvar_inv\n",
    "    # batch norm layer output with gain and bias (scale and shift)\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # activation function\n",
    "    h = torch.tanh(hpreact)\n",
    "\n",
    "    # ----- Linear Layer 2 -----\n",
    "    # logits\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    # ----- Cross Entropy Loss -----\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    # ----- Backpropagation -----\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # one expression\n",
    "    dlogits = F.softmax(logits, dim = 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "\n",
    "    # z = x * y ➡️ dz/dx = y (transpose because of matrix multiplication)\n",
    "    dh = dlogits @ W2.T\n",
    "\n",
    "    # z = x * y ➡️ dz/dx = x (transpose because of matrix multiplication)\n",
    "    dW2 = h.T @ dlogits\n",
    "\n",
    "    # z = x * y + a ➡️ dz/da = 1 (sum along the batch_size axis)\n",
    "    db2 = dlogits.sum(0, keepdim = False)\n",
    "\n",
    "    # z = tanh(x) ➡️ dz/dx = 1 - tanh(x)^2 = 1 - z^2\n",
    "    dhpreact = dh * (1.0 - h ** 2)\n",
    "\n",
    "    # z = x * y + a ➡️ dz/dx = y (transpose because of matrix multiplication)\n",
    "    dbngain = (dhpreact * bnraw).sum(0, keepdim = True)\n",
    "\n",
    "    # z = x * y + a ➡️ dz/da = 1 (sum along the batch_size axis)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "    # one expression\n",
    "    dhprebn = bngain * bnvar_inv / n * (n * dhpreact - dhpreact.sum(0) - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0))\n",
    "\n",
    "    # z = x * y + a ➡️ dz/dx = y (transpose because of matrix multiplication)\n",
    "    dembcat = dhprebn @ W1.T\n",
    "\n",
    "    # z = x * y + a ➡️ dz/dx = y (transpose because of matrix multiplication)\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "\n",
    "    # z = x * y + a ➡️ dz/da = 1 (sum along the batch_size axis)\n",
    "    db1 = dhprebn.sum(0, keepdim = False)\n",
    "\n",
    "    # just reshape the gradient to the shape of the embedding\n",
    "    demb = dembcat.view_as(emb)\n",
    "\n",
    "    # we need to undo the embedding lookup (the indexing)\n",
    "    dC = torch.zeros_like(C)\n",
    "    # iterate over Xb rows\n",
    "    for k in range(Xb.shape[0]):\n",
    "        # iterate over Xb columns\n",
    "        for j in range(Xb.shape[1]):\n",
    "            # get the index of the current character\n",
    "            ix = Xb[k, j]\n",
    "            # add the gradient to the corresponding row of the gradient matrix\n",
    "            dC[ix] += demb[k, j]\n",
    "            \n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "    # update the parameters\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p, grad in zip(params, grads):\n",
    "        p.data -= lr * grad\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Step {i}, Loss {loss.item()}\")\n",
    "        lossi.append(loss.item())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchnorm calibration\n",
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    bnmean = hpreact.mean(0, keepdim = True)\n",
    "    bnvar = hpreact.var(0, keepdim = True, unbiased = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.1448161602020264\n",
      "dev loss: 2.1594536304473877\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        \"train\": (Xtr, Ytr),\n",
    "        \"dev\": (Xdev, Ydev),\n",
    "        \"test\": (Xte, Yte),\n",
    "    }[split]\n",
    "\n",
    "    emb = C[x] # (N, block_size, n_embd)\n",
    "    embcat = emb.view((emb.shape[0], -1)) # (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1 # (N, n_hidden)\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    print(f\"{split} loss: {loss.item()}\")\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mora.\n",
      "kmyah.\n",
      "see.\n",
      "mad.\n",
      "rylla.\n",
      "emmastendraeg.\n",
      "adelynneli.\n",
      "jemi.\n",
      "jen.\n",
      "eden.\n",
      "sananar.\n",
      "kayzion.\n",
      "kalin.\n",
      "shubergias.\n",
      "jest.\n",
      "jair.\n",
      "jennex.\n",
      "teron.\n",
      "ubelled.\n",
      "ryyah.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
